{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c rmg pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def build_squares(img):\n",
    "\tx, y, w, h = 420, 140, 10, 10\n",
    "\td = 10\n",
    "\timgCrop = None\n",
    "\tcrop = None\n",
    "\tfor i in range(10):\n",
    "\t\tfor j in range(5):\n",
    "\t\t\tif np.any(imgCrop == None):\n",
    "\t\t\t\timgCrop = img[y:y+h, x:x+w]\n",
    "\t\t\telse:\n",
    "\t\t\t\timgCrop = np.hstack((imgCrop, img[y:y+h, x:x+w]))\n",
    "\t\t\t#print(imgCrop.shape)\n",
    "\t\t\tcv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\t\t\tx+=w+d\n",
    "\t\tif np.any(crop == None):\n",
    "\t\t\tcrop = imgCrop\n",
    "\t\telse:\n",
    "\t\t\tcrop = np.vstack((crop, imgCrop)) \n",
    "\t\timgCrop = None\n",
    "\t\tx = 420\n",
    "\t\ty+=h+d\n",
    "\treturn crop\n",
    "\n",
    "def get_hand_hist():\n",
    "\tcam = cv2.VideoCapture(1)\n",
    "\tif cam.read()[0]==False:\n",
    "\t\tcam = cv2.VideoCapture(0)\n",
    "\tx, y, w, h = 300, 100, 300, 300\n",
    "\tflagPressedC, flagPressedS = False, False\n",
    "\timgCrop = None\n",
    "\twhile True:\n",
    "\t\timg = cam.read()[1]\n",
    "\t\timg = cv2.flip(img, 1)\n",
    "\t\timg = cv2.resize(img, (640, 480))\n",
    "\t\thsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\t\t\n",
    "\t\tkeypress = cv2.waitKey(1)\n",
    "\t\tif keypress == ord('c'):\t\t\n",
    "\t\t\thsvCrop = cv2.cvtColor(imgCrop, cv2.COLOR_BGR2HSV)\n",
    "\t\t\tflagPressedC = True\n",
    "\t\t\thist = cv2.calcHist([hsvCrop], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "\t\t\tcv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\t\telif keypress == ord('s'):\n",
    "\t\t\tflagPressedS = True\t\n",
    "\t\t\tbreak\n",
    "\t\tif flagPressedC:\t\n",
    "\t\t\tdst = cv2.calcBackProject([hsv], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "\t\t\tdst1 = dst.copy()\n",
    "\t\t\tdisc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "\t\t\tcv2.filter2D(dst,-1,disc,dst)\n",
    "\t\t\tblur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "\t\t\tblur = cv2.medianBlur(blur, 15)\n",
    "\t\t\tret,thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\t\t\tthresh = cv2.merge((thresh,thresh,thresh))\n",
    "\t\t\t#cv2.imshow(\"res\", res)\n",
    "\t\t\tcv2.imshow(\"Thresh\", thresh)\n",
    "\t\tif not flagPressedS:\n",
    "\t\t\timgCrop = build_squares(img)\n",
    "\t\t#cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "\t\tcv2.imshow(\"Set hand histogram\", img)\n",
    "\tcam.release()\n",
    "\tcv2.destroyAllWindows()\n",
    "\twith open(\"hist\", \"wb\") as f:\n",
    "\t\tpickle.dump(hist, f)\n",
    "\n",
    "\n",
    "get_hand_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle, os, sqlite3, random\n",
    "\n",
    "image_x, image_y = 50, 50\n",
    "\n",
    "def get_hand_hist():\n",
    "\twith open(\"hist\", \"rb\") as f:\n",
    "\t\thist = pickle.load(f)\n",
    "\treturn hist\n",
    "\n",
    "def init_create_folder_database():\n",
    "\t# create the folder and database if not exist\n",
    "\tif not os.path.exists(\"gestures\"):\n",
    "\t\tos.mkdir(\"gestures\")\n",
    "\tif not os.path.exists(\"gesture_db.db\"):\n",
    "\t\tconn = sqlite3.connect(\"gesture_db.db\")\n",
    "\t\tcreate_table_cmd = \"CREATE TABLE gesture ( g_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, g_name TEXT NOT NULL )\"\n",
    "\t\tconn.execute(create_table_cmd)\n",
    "\t\tconn.commit()\n",
    "\n",
    "def create_folder(folder_name):\n",
    "\tif not os.path.exists(folder_name):\n",
    "\t\tos.mkdir(folder_name)\n",
    "\n",
    "def store_in_db(g_id, g_name):\n",
    "\tconn = sqlite3.connect(\"gesture_db.db\")\n",
    "\tcmd = \"INSERT INTO gesture (g_id, g_name) VALUES (%s, \\'%s\\')\" % (g_id, g_name)\n",
    "\ttry:\n",
    "\t\tconn.execute(cmd)\n",
    "\texcept sqlite3.IntegrityError:\n",
    "\t\tchoice = input(\"g_id already exists. Want to change the record? (y/n): \")\n",
    "\t\tif choice.lower() == 'y':\n",
    "\t\t\tcmd = \"UPDATE gesture SET g_name = \\'%s\\' WHERE g_id = %s\" % (g_name, g_id)\n",
    "\t\t\tconn.execute(cmd)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Doing nothing...\")\n",
    "\t\t\treturn\n",
    "\tconn.commit()\n",
    "\t\n",
    "def store_images(g_id):\n",
    "\ttotal_pics = 500\n",
    "\thist = get_hand_hist()\n",
    "\tcam = cv2.VideoCapture(0)\n",
    "\tif cam.read()[0]==False:\n",
    "\t\tcam = cv2.VideoCapture(0)\n",
    "\tx, y, w, h = 300, 100, 300, 300\n",
    "\n",
    "\tcreate_folder(\"gestures/\"+str(g_id))\n",
    "\tpic_no = 0\n",
    "\tflag_start_capturing = False\n",
    "\tframes = 0\n",
    "\t\n",
    "\twhile True:\n",
    "\t\timg = cam.read()[1]\n",
    "\t\timg = cv2.flip(img, 1)\n",
    "\t\timgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\t\tdst = cv2.calcBackProject([imgHSV], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "\t\tdisc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "\t\tcv2.filter2D(dst,-1,disc,dst)\n",
    "\t\tblur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "\t\tblur = cv2.medianBlur(blur, 15)\n",
    "\t\tthresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "\t\tthresh = cv2.merge((thresh,thresh,thresh))\n",
    "\t\tthresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "\t\tthresh = thresh[y:y+h, x:x+w]\n",
    "\t\t# 1. it worked -  CHANGED THIS NEXT 2 LINEs OF CODE >> \n",
    "\t\ttmp = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\t\tcontours = tmp[0] if len(tmp) == 2 else tmp[1]      \n",
    "\t\t# CHANGED THIS NEXT 2 LINEs OF CODE <<          \n",
    "\n",
    "\t\tif len(contours) > 0:\n",
    "\t\t\tcontour = max(contours, key = cv2.contourArea)\n",
    "\t\t\tif cv2.contourArea(contour) > 10000 and frames > 50:\n",
    "\t\t\t\tx1, y1, w1, h1 = cv2.boundingRect(contour)\n",
    "\t\t\t\tpic_no += 1\n",
    "\t\t\t\tsave_img = thresh[y1:y1+h1, x1:x1+w1]\n",
    "\t\t\t\tif w1 > h1:\n",
    "\t\t\t\t\tsave_img = cv2.copyMakeBorder(save_img, int((w1-h1)/2) , int((w1-h1)/2) , 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "\t\t\t\telif h1 > w1:\n",
    "\t\t\t\t\tsave_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1-w1)/2) , int((h1-w1)/2) , cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "\t\t\t\tsave_img = cv2.resize(save_img, (image_x, image_y))\n",
    "\t\t\t\trand = random.randint(0, 10)\n",
    "\t\t\t\tif rand % 2 == 0:\n",
    "\t\t\t\t\tsave_img = cv2.flip(save_img, 1)\n",
    "\t\t\t\tcv2.putText(img, \"Capturing...\", (30, 60), cv2.FONT_HERSHEY_TRIPLEX, 2, (127, 255, 255))\n",
    "\t\t\t\tcv2.imwrite(\"gestures/\"+str(g_id)+\"/\"+str(pic_no)+\".jpg\", save_img)\n",
    "\n",
    "\t\tcv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "\t\tcv2.putText(img, str(pic_no), (30, 400), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (127, 127, 255))\n",
    "\t\tcv2.imshow(\"Capturing gesture\", img)\n",
    "\t\tcv2.imshow(\"thresh\", thresh)\n",
    "\t\tkeypress = cv2.waitKey(1)\n",
    "\t\tif keypress == ord('c'):\n",
    "\t\t\tif flag_start_capturing == False:\n",
    "\t\t\t\tflag_start_capturing = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tflag_start_capturing = False\n",
    "\t\t\t\tframes = 0\n",
    "\t\tif flag_start_capturing == True:\n",
    "\t\t\tframes += 1\n",
    "\t\tif pic_no == total_pics:\n",
    "\t\t\tbreak\n",
    "            \n",
    "\t\t# 2. test - CHANGED THIS NEXT 2 LINEs OF CODE          \n",
    "\t# cam.release()\n",
    "\tcv2.destroyAllWindows()\n",
    "\t\t# CHANGED THIS NEXT 2 LINEs OF CODE >>\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "init_create_folder_database()\n",
    "g_id = input(\"Enter gesture no.: \")\n",
    "g_name = input(\"Enter gesture name/text: \")\n",
    "store_in_db(g_id, g_name)\n",
    "store_images(g_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "def flip_images():\n",
    "\tgest_folder = \"gestures\"\n",
    "\timages_labels = []\n",
    "\timages = []\n",
    "\tlabels = []\n",
    "\tfor g_id in os.listdir(gest_folder):\n",
    "\t\tfor i in range(500):\n",
    "\t\t\tpath = gest_folder+\"/\"+g_id+\"/\"+str(i+1)+\".jpg\"\n",
    "\t\t\tnew_path = gest_folder+\"/\"+g_id+\"/\"+str(i+1+500)+\".jpg\"\n",
    "\t\t\tprint(path)\n",
    "\t\t\timg = cv2.imread(path, 0)\n",
    "\t\t\timg = cv2.flip(img, 1)\n",
    "\t\t\tcv2.imwrite(new_path, img)\n",
    "\n",
    "flip_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def pickle_images_labels():\n",
    "\timages_labels = []\n",
    "\timages = glob(\"gestures/*/*.jpg\")\n",
    "\timages.sort()\n",
    "\tfor image in images:\n",
    "\t\tprint(image)\n",
    "\t\tlabel = image[image.find(os.sep)+1: image.rfind(os.sep)]\n",
    "\t\timg = cv2.imread(image, 0)\n",
    "\t\timages_labels.append((np.array(img, dtype=np.uint8), int(label)))\n",
    "\treturn images_labels\n",
    "\n",
    "images_labels = pickle_images_labels()\n",
    "images_labels = shuffle(shuffle(shuffle(shuffle(images_labels))))\n",
    "images, labels = zip(*images_labels)\n",
    "print(\"Length of images_labels\", len(images_labels))\n",
    "\n",
    "train_images = images[:int(5/6*len(images))]\n",
    "print(\"Length of train_images\", len(train_images))\n",
    "with open(\"train_images\", \"wb\") as f:\n",
    "\tpickle.dump(train_images, f)\n",
    "del train_images\n",
    "\n",
    "train_labels = labels[:int(5/6*len(labels))]\n",
    "print(\"Length of train_labels\", len(train_labels))\n",
    "with open(\"train_labels\", \"wb\") as f:\n",
    "\tpickle.dump(train_labels, f)\n",
    "del train_labels\n",
    "\n",
    "test_images = images[int(5/6*len(images)):int(11/12*len(images))]\n",
    "print(\"Length of test_images\", len(test_images))\n",
    "with open(\"test_images\", \"wb\") as f:\n",
    "\tpickle.dump(test_images, f)\n",
    "del test_images\n",
    "\n",
    "test_labels = labels[int(5/6*len(labels)):int(11/12*len(images))]\n",
    "print(\"Length of test_labels\", len(test_labels))\n",
    "with open(\"test_labels\", \"wb\") as f:\n",
    "\tpickle.dump(test_labels, f)\n",
    "del test_labels\n",
    "\n",
    "val_images = images[int(11/12*len(images)):]\n",
    "print(\"Length of test_images\", len(val_images))\n",
    "with open(\"val_images\", \"wb\") as f:\n",
    "\tpickle.dump(val_images, f)\n",
    "del val_images\n",
    "\n",
    "val_labels = labels[int(11/12*len(labels)):]\n",
    "print(\"Length of val_labels\", len(val_labels))\n",
    "with open(\"val_labels\", \"wb\") as f:\n",
    "\tpickle.dump(val_labels, f)\n",
    "del val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, random\n",
    "import numpy as np\n",
    "\n",
    "def get_image_size():\n",
    "\timg = cv2.imread('gestures/0/1.jpg', 0)\n",
    "\treturn img.shape\n",
    "\n",
    "gestures = os.listdir('gestures/')\n",
    "gestures.sort(key = int)\n",
    "begin_index = 0\n",
    "end_index = 5\n",
    "image_x, image_y = get_image_size()\n",
    "\n",
    "if len(gestures)%5 != 0:\n",
    "\trows = int(len(gestures)/5)+1\n",
    "else:\n",
    "\trows = int(len(gestures)/5)\n",
    "\n",
    "full_img = None\n",
    "for i in range(rows):\n",
    "\tcol_img = None\n",
    "\tfor j in range(begin_index, end_index):\n",
    "\t\timg_path = \"gestures/%s/%d.jpg\" % (j, random.randint(1, 500))\n",
    "\t\timg = cv2.imread(img_path, 0)\n",
    "\t\tif np.any(img == None):\n",
    "\t\t\timg = np.zeros((image_y, image_x), dtype = np.uint8)\n",
    "\t\tif np.any(col_img == None):\n",
    "\t\t\tcol_img = img\n",
    "\t\telse:\n",
    "\t\t\tcol_img = np.hstack((col_img, img))\n",
    "\n",
    "\tbegin_index += 5\n",
    "\tend_index += 5\n",
    "\tif np.any(full_img == None):\n",
    "\t\tfull_img = col_img\n",
    "\telse:\n",
    "\t\tfull_img = np.vstack((full_img, col_img))\n",
    "\n",
    "\n",
    "cv2.imshow(\"gestures\", full_img)\n",
    "cv2.imwrite('full_img.jpg', full_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2, os\n",
    "from glob import glob\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def get_image_size():\n",
    "\timg = cv2.imread('gestures/0/1.jpg', 0)\n",
    "\treturn img.shape\n",
    "\n",
    "def get_num_of_classes():\n",
    "\treturn len(glob('gestures/*'))\n",
    "\n",
    "image_x, image_y = get_image_size()\n",
    "\n",
    "def cnn_model():\n",
    "\tnum_of_classes = get_num_of_classes()\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(16, (2,2), input_shape=(image_x, image_y, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\tmodel.add(Conv2D(32, (3,3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
    "\tmodel.add(Conv2D(64, (5,5), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(num_of_classes, activation='softmax'))\n",
    "\tsgd = optimizers.SGD(lr=1e-2)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\tfilepath=\"cnn_model_keras2.h5\"\n",
    "\tcheckpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\tcallbacks_list = [checkpoint1]\n",
    "\tfrom keras.utils import plot_model\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model, callbacks_list\n",
    "\n",
    "def train():\n",
    "\twith open(\"train_images\", \"rb\") as f:\n",
    "\t\ttrain_images = np.array(pickle.load(f))\n",
    "\twith open(\"train_labels\", \"rb\") as f:\n",
    "\t\ttrain_labels = np.array(pickle.load(f), dtype=np.int32)\n",
    "\n",
    "\twith open(\"val_images\", \"rb\") as f:\n",
    "\t\tval_images = np.array(pickle.load(f))\n",
    "\twith open(\"val_labels\", \"rb\") as f:\n",
    "\t\tval_labels = np.array(pickle.load(f), dtype=np.int32)\n",
    "\n",
    "\ttrain_images = np.reshape(train_images, (train_images.shape[0], image_x, image_y, 1))\n",
    "\tval_images = np.reshape(val_images, (val_images.shape[0], image_x, image_y, 1))\n",
    "\ttrain_labels = np_utils.to_categorical(train_labels)\n",
    "\tval_labels = np_utils.to_categorical(val_labels)\n",
    "\n",
    "\tprint(val_labels.shape)\n",
    "\n",
    "\tmodel, callbacks_list = cnn_model()\n",
    "\tmodel.summary()\n",
    "\tmodel.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=100, batch_size=500, callbacks=callbacks_list)\n",
    "\tscores = model.evaluate(val_images, val_labels, verbose=0)\n",
    "\tprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\t#model.save('cnn_model_keras2.h5')\n",
    "\n",
    "train()\n",
    "K.clear_session();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "    title:        the text to display at the top of the matrix\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "\n",
    "\n",
    "image_x, image_y = 50, 50\n",
    "with open(\"test_images\", \"rb\") as f:\n",
    "\ttest_images = np.array(pickle.load(f))\n",
    "with open(\"test_labels\", \"rb\") as f:\n",
    "\ttest_labels = np.array(pickle.load(f), dtype=np.int32)\n",
    "test_images = np.reshape(test_images, (test_images.shape[0], image_x, image_y, 1))\n",
    "\n",
    "\n",
    "model = load_model('cnn_model_keras2.h5')\n",
    "pred_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "pred_probabs = model.predict(test_images)\n",
    "end_time = time.time()\n",
    "pred_time = end_time-start_time\n",
    "avg_pred_time = pred_time/test_images.shape[0]\n",
    "print(\"Time taken to predict %d test images is %ds\" %(test_images.shape[0], pred_time))\n",
    "print('Average prediction time: %fs' % (avg_pred_time))\n",
    "\n",
    "for pred_probab in pred_probabs:\n",
    "\tpred_labels.append(list(pred_probab).index(max(pred_probab)))\n",
    "\n",
    "cm = confusion_matrix(test_labels, np.array(pred_labels))\n",
    "classification_report = classification_report(test_labels, np.array(pred_labels))\n",
    "print('\\n\\nClassification Report') \n",
    "print('---------------------------')\n",
    "print(classification_report)\n",
    "plot_confusion_matrix(cm, range(44), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def build_squares(img):\n",
    "\tx, y, w, h = 420, 140, 10, 10\n",
    "\td = 10\n",
    "\timgCrop = None\n",
    "\tcrop = None\n",
    "\tfor i in range(10):\n",
    "\t\tfor j in range(5):\n",
    "\t\t\tif np.any(imgCrop == None):\n",
    "\t\t\t\timgCrop = img[y:y+h, x:x+w]\n",
    "\t\t\telse:\n",
    "\t\t\t\timgCrop = np.hstack((imgCrop, img[y:y+h, x:x+w]))\n",
    "\t\t\t#print(imgCrop.shape)\n",
    "\t\t\tcv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\t\t\tx+=w+d\n",
    "\t\tif np.any(crop == None):\n",
    "\t\t\tcrop = imgCrop\n",
    "\t\telse:\n",
    "\t\t\tcrop = np.vstack((crop, imgCrop)) \n",
    "\t\timgCrop = None\n",
    "\t\tx = 420\n",
    "\t\ty+=h+d\n",
    "\treturn crop\n",
    "\n",
    "def get_hand_hist():\n",
    "\tcam = cv2.VideoCapture(1)\n",
    "\tif cam.read()[0]==False:\n",
    "\t\tcam = cv2.VideoCapture(0)\n",
    "\tx, y, w, h = 300, 100, 300, 300\n",
    "\tflagPressedC, flagPressedS = False, False\n",
    "\timgCrop = None\n",
    "\twhile True:\n",
    "\t\timg = cam.read()[1]\n",
    "\t\timg = cv2.flip(img, 1)\n",
    "\t\timg = cv2.resize(img, (640, 480))\n",
    "\t\thsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\t\t\n",
    "\t\tkeypress = cv2.waitKey(1)\n",
    "\t\tif keypress == ord('c'):\t\t\n",
    "\t\t\thsvCrop = cv2.cvtColor(imgCrop, cv2.COLOR_BGR2HSV)\n",
    "\t\t\tflagPressedC = True\n",
    "\t\t\thist = cv2.calcHist([hsvCrop], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "\t\t\tcv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\t\telif keypress == ord('s'):\n",
    "\t\t\tflagPressedS = True\t\n",
    "\t\t\tbreak\n",
    "\t\tif flagPressedC:\t\n",
    "\t\t\tdst = cv2.calcBackProject([hsv], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "\t\t\tdst1 = dst.copy()\n",
    "\t\t\tdisc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "\t\t\tcv2.filter2D(dst,-1,disc,dst)\n",
    "\t\t\tblur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "\t\t\tblur = cv2.medianBlur(blur, 15)\n",
    "\t\t\tret,thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\t\t\tthresh = cv2.merge((thresh,thresh,thresh))\n",
    "\t\t\t#cv2.imshow(\"res\", res)\n",
    "\t\t\tcv2.imshow(\"Thresh\", thresh)\n",
    "\t\tif not flagPressedS:\n",
    "\t\t\timgCrop = build_squares(img)\n",
    "\t\t#cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "\t\tcv2.imshow(\"Set hand histogram\", img)\n",
    "\tcam.release()\n",
    "\tcv2.destroyAllWindows()\n",
    "\twith open(\"hist\", \"wb\") as f:\n",
    "\t\tpickle.dump(hist, f)\n",
    "\n",
    "\n",
    "get_hand_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sqlite3, pyttsx3\n",
    "from keras.models import load_model\n",
    "from threading import Thread\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "model = load_model('cnn_model_keras2.h5')\n",
    "\n",
    "def get_image_size():\n",
    "\timg = cv2.imread('gestures/0/1.jpg', 0)\n",
    "\treturn img.shape\n",
    "\n",
    "image_x, image_y = get_image_size()\n",
    "\n",
    "\n",
    "def keras_process_image(img):\n",
    "\timg = cv2.resize(img, (image_x, image_y))\n",
    "\timg = np.array(img, dtype=np.float32)\n",
    "\timg = np.reshape(img, (1, image_x, image_y, 1))\n",
    "\treturn img\n",
    "\n",
    "def keras_predict(model, image):\n",
    "\tprocessed = keras_process_image(image)\n",
    "\tpred_probab = model.predict(processed)[0]\n",
    "\tpred_class = list(pred_probab).index(max(pred_probab))\n",
    "\treturn max(pred_probab), pred_class\n",
    "\n",
    "def get_pred_text_from_db(pred_class):\n",
    "\tconn = sqlite3.connect(\"gesture_db.db\")\n",
    "\tcmd = \"SELECT g_name FROM gesture WHERE g_id=\"+str(pred_class)\n",
    "\tcursor = conn.execute(cmd)\n",
    "\tfor row in cursor:\n",
    "\t\treturn row[0]\n",
    "\n",
    "def split_sentence(text, num_of_words):\n",
    "\t'''\n",
    "\tSplits a text into group of num_of_words\n",
    "\t'''\n",
    "\tlist_words = text.split(\" \")\n",
    "\tlength = len(list_words)\n",
    "\tsplitted_sentence = []\n",
    "\tb_index = 0\n",
    "\te_index = num_of_words\n",
    "\twhile length > 0:\n",
    "\t\tpart = \"\"\n",
    "\t\tfor word in list_words[b_index:e_index]:\n",
    "\t\t\tpart = part + \" \" + word\n",
    "\t\tsplitted_sentence.append(part)\n",
    "\t\tb_index += num_of_words\n",
    "\t\te_index += num_of_words\n",
    "\t\tlength -= num_of_words\n",
    "\treturn splitted_sentence\n",
    "\n",
    "def put_splitted_text_in_blackboard(blackboard, splitted_text):\n",
    "\ty = 200\n",
    "\tfor text in splitted_text:\n",
    "\t\tcv2.putText(blackboard, text, (4, y), cv2.FONT_HERSHEY_TRIPLEX, 2, (255, 255, 255))\n",
    "\t\ty += 50\n",
    "\n",
    "def get_hand_hist():\n",
    "\twith open(\"hist\", \"rb\") as f:\n",
    "\t\thist = pickle.load(f)\n",
    "\treturn hist\n",
    "\n",
    "def recognize():\n",
    "\tglobal prediction\n",
    "\tcam = cv2.VideoCapture(1)\n",
    "\tif cam.read()[0] == False:\n",
    "\t\tcam = cv2.VideoCapture(0)\n",
    "\thist = get_hand_hist()\n",
    "\tx, y, w, h = 300, 100, 300, 300\n",
    "\twhile True:\n",
    "\t\ttext = \"\"\n",
    "\t\timg = cam.read()[1]\n",
    "\t\timg = cv2.flip(img, 1)\n",
    "\t\timg = cv2.resize(img, (640, 480))\n",
    "\t\timgCrop = img[y:y+h, x:x+w]\n",
    "\t\timgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\t\tdst = cv2.calcBackProject([imgHSV], [0, 1], hist, [0, 180, 0, 256], 1)\n",
    "\t\tdisc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "\t\tcv2.filter2D(dst,-1,disc,dst)\n",
    "\t\tblur = cv2.GaussianBlur(dst, (11,11), 0)\n",
    "\t\tblur = cv2.medianBlur(blur, 15)\n",
    "\t\tthresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "\t\tthresh = cv2.merge((thresh,thresh,thresh))\n",
    "\t\tthresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "\t\tthresh = thresh[y:y+h, x:x+w]\n",
    "\t\tcontours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[0]\n",
    "\t\tif len(contours) > 0:\n",
    "\t\t\tcontour = max(contours, key = cv2.contourArea)\n",
    "\t\t\t#print(cv2.contourArea(contour))\n",
    "\t\t\tif cv2.contourArea(contour) > 10000:\n",
    "\t\t\t\tx1, y1, w1, h1 = cv2.boundingRect(contour)\n",
    "\t\t\t\tsave_img = thresh[y1:y1+h1, x1:x1+w1]\n",
    "\t\t\t\t\n",
    "\t\t\t\tif w1 > h1:\n",
    "\t\t\t\t\tsave_img = cv2.copyMakeBorder(save_img, int((w1-h1)/2) , int((w1-h1)/2) , 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "\t\t\t\telif h1 > w1:\n",
    "\t\t\t\t\tsave_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1-w1)/2) , int((h1-w1)/2) , cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "\t\t\t\t\n",
    "\t\t\t\tpred_probab, pred_class = keras_predict(model, save_img)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif pred_probab*100 > 80:\n",
    "\t\t\t\t\ttext = get_pred_text_from_db(pred_class)\n",
    "\t\t\t\t\tprint(text)\n",
    "\t\tblackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "\t\tsplitted_text = split_sentence(text, 2)\n",
    "\t\tput_splitted_text_in_blackboard(blackboard, splitted_text)\n",
    "\t\t#cv2.putText(blackboard, text, (30, 200), cv2.FONT_HERSHEY_TRIPLEX, 1.3, (255, 255, 255))\n",
    "\t\tcv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "\t\tres = np.hstack((img, blackboard))\n",
    "\t\tcv2.imshow(\"Recognizing gesture\", res)\n",
    "\t\tcv2.imshow(\"thresh\", thresh)\n",
    "\t\tif cv2.waitKey(1) == ord('q'):\n",
    "\t\t\tbreak\n",
    "\n",
    "keras_predict(model, np.zeros((50, 50), dtype=np.uint8))\t\t\n",
    "recognize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
